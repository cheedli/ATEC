{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "wX5Zcme520SD"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wX5Zcme520SD"
      },
      "source": [
        "# **Install Dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAacq0t0ENF9",
        "outputId": "900d76b2-9a6c-4b51-a8d7-4340bb02046b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.6/380.6 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.5/322.5 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.0/817.0 kB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.5/149.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.9/586.9 kB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.5/294.5 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.0/284.0 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.37.2 optimum==1.12.0 --quiet\n",
        "!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/ --quiet\n",
        "!pip install langchain==0.1.9 --quiet\n",
        "# !pip install chromadb\n",
        "!pip install sentence_transformers==2.4.0 --quiet\n",
        "!pip install unstructured --quiet\n",
        "!pip install pdf2image --quiet\n",
        "!pip install pdfminer.six==20221105 --quiet\n",
        "!pip install unstructured-inference --quiet\n",
        "!pip install faiss-gpu==1.7.2 --quiet\n",
        "!pip install pikepdf==8.13.0 --quiet\n",
        "!pip install pypdf==4.0.2 --quiet\n",
        "!pip install pillow_heif==0.15.0 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88O0EYHAomrp",
        "outputId": "1a03f644-57ba-42de-a187-240df2bd36b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: auto-gptq in /usr/local/lib/python3.10/dist-packages (0.7.1+cu118)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.34.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (3.0.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (1.26.4)\n",
            "Requirement already satisfied: rouge in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (1.0.1)\n",
            "Requirement already satisfied: gekko in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (1.2.1)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (2.4.1+cu121)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.4.5)\n",
            "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (4.37.2)\n",
            "Requirement already satisfied: peft>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.13.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (4.66.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq) (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (2024.6.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (0.15.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (3.10.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge->auto-gptq) (1.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->auto-gptq) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2024.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->auto-gptq) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "#pip install optimum\n",
        "!pip install auto-gptq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWfHSyaOn4Wj"
      },
      "source": [
        "# **Load Llama 2**\n",
        "*We will use the quantized version of the LLAMA 2 13B model from HuggingFace for our RAG task.*\n",
        "\n",
        "**PS: Don't forget to restart Runtime before running the upcomming cells!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440,
          "referenced_widgets": [
            "fd947a5df0114b78a5271c963571517b",
            "702e5ad79da54d01bf1fac0674d4968c",
            "598e9b4c0d2049758be0d049e4f465cc",
            "ed05b67162b9417f9a56f9373cf609af",
            "98ea78ceecea42388e69840d868f9c77",
            "5c262a87e560450893cddc5c8619a557",
            "344f62974c784eb1b8f2745d832d74b5",
            "bffbb651f14b47eda9c16cacc42dd0fe",
            "169c56b0fc1347dc8e03fe813dfacb0a",
            "83f160922cd14c8d8151105928bc295b",
            "af5cea50af324723903bdfda4524d068",
            "5ef57e631e1444a3bc2fd506b4bc2d6a",
            "c73218c3787f48ba956b1d992cea5f2e",
            "538d88faa2674227835fcd305671d8e1",
            "be293861eaf845959759bcda90e91fea",
            "f388e990a705410a9f8f2258dd43eb9d",
            "4ef9b04c61d942ff806b05068408255a",
            "37bdf4dd33d7433ca05541d2f650427d",
            "55f132feb8694e9b9d9eef6670a15d3d",
            "e97d57b3ed5e4ba29507faff536b69d3",
            "980d4b16f4074cf49ec7376a0c159a75",
            "4110b3959055453188abedb2264f381b",
            "0e2492392cd944ffa6dc23a2a81b9c96",
            "37b3faf23de7478d933a3ec900c6faad",
            "e7c74a121c004dd2804bd129fdee76d6",
            "65e75d6a3b3f41dfa512ec0b6d25cee1",
            "82d70aa9a9994f3eaf1dcfc3dd8dcef8",
            "7380fba077854c4faf6e535dd5a7329d",
            "5550b513ef644f0b9d527c03f835e0e5",
            "b18988dabe7944fe8f45b53de0a7992b",
            "598b51a911bf4450baff210d6511f743",
            "65ea05ff38bc42ffb035082c8816a221",
            "3cc20b1890d54a9baaed14871087b952",
            "acd9d19566f34f6f9b7cb18408ca2be2",
            "c31c09c634cd4f638389ae75354c5954",
            "08457e430de748418bc4c3873e91a679",
            "ca24eb6f574a40f9b4db4606f0f3f70e",
            "b5e6a606201f4389ab1a5328efb33d1f",
            "11fd44e7c1da40f9a0ef22a7c7d47972",
            "fe50a42967ba4559a87d762355ed189c",
            "1821fffe2bfc431db4be45d4ba9c3625",
            "da9af3e37a9146ed9195a5c1e9f5919c",
            "c201ea1d8e624bdb83cbcb81573e931d",
            "0b6cc9143b86429cbe5c9fad85d4ba13",
            "72222de6e36c4b52ba757cb6adb0ef7f",
            "b4830ac676294461b82522b5142e0991",
            "6427e82ec45445229786fc3dfb3c5523",
            "1782053b29b34df488d1b0a41f6cdf1b",
            "b62ae23419ec447890047c1bbe3a0919",
            "91021e9946494bb69147dc259a8dcc69",
            "32ec9dad0b684d75901fb949648d394f",
            "73c3f7fc185e424e9460ef881266836b",
            "e6f2c02dc2ad4c0d8be12e8a76345070",
            "68c38bf78d9c4e8f963ecdc21392c5d4",
            "12504fa67d574d38a933ca90593884df",
            "8eda95fa345346b893ac2698681a6375",
            "c5af6f7b675c4b11a5caec0b229c7249",
            "8d8e302ffccd4cb89aa0108ba6273df5",
            "b746f489a6c14fdaa5836ee1583db51a",
            "7d26ae53cbda4e49812148470f0032d5",
            "de5e7160db0d4c15bf3e52d9464382a5",
            "44161c2e21ae4b3b9016e29b05e9d82d",
            "a34094c42f284201ba0a4a4b5eb5eaa9",
            "51b4a3997256484fa929562ad5fd3a21",
            "d2f490ed935d4b9f91bd85b0080b7f97",
            "a9026549f36245af8b176f1db1ca0616",
            "00d08df7c2674ca18bd306e3a5e56c61",
            "5c91bbb5095142d483dab48be5c2b9cf",
            "f14c263054ad437e825ff5d86cc9ca2a",
            "050ac6d35b4c472cafd31cbebb0307ab",
            "6adff85b81fe48378b5d0537394d1032",
            "8175c116858049669214dbd59cce1ad3",
            "6be5461024894b5e8c2007e0510c46d6",
            "76055efe092c415390ac11d34e409fcc",
            "4ef115b59e4d415b9b3b349b592cefa3",
            "ea725c41421f4eadb9ed0f31627fda85",
            "a4a858f7648d4aba9bb8c6325e9d9168"
          ]
        },
        "id": "m8daoLBnnr1q",
        "outputId": "e247ec15-5cd1-4f1b-f9dd-ada97de8e855"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fd947a5df0114b78a5271c963571517b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/837 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:auto_gptq.nn_modules.qlinear.qlinear_cuda:CUDA extension not installed.\n",
            "WARNING:auto_gptq.nn_modules.qlinear.qlinear_cuda_old:CUDA extension not installed.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ef57e631e1444a3bc2fd506b4bc2d6a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/7.26G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0e2492392cd944ffa6dc23a2a81b9c96",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "acd9d19566f34f6f9b7cb18408ca2be2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72222de6e36c4b52ba757cb6adb0ef7f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8eda95fa345346b893ac2698681a6375",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "00d08df7c2674ca18bd306e3a5e56c61",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
        "\n",
        "model_name = \"TheBloke/Llama-2-13b-Chat-GPTQ\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             device_map=\"auto\",\n",
        "                                             trust_remote_code=True)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "gen_cfg = GenerationConfig.from_pretrained(model_name)\n",
        "gen_cfg.max_new_tokens=512\n",
        "gen_cfg.temperature=0.0000001 # 0.0\n",
        "gen_cfg.return_full_text=True\n",
        "gen_cfg.do_sample=True\n",
        "gen_cfg.repetition_penalty=1.11\n",
        "\n",
        "pipe=pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    generation_config=gen_cfg\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wlfU-CNoEFF"
      },
      "source": [
        "*Test LLM with Llama 2 prompt structure and LangChain PromptTemplate*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6dh-PC1toDoG",
        "outputId": "49c6d5a6-1581-490a-b492-1cdb804d9db6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INST] <> You are an AI assistant. You are truthful, unbiased and honest in your response.  If you\n",
            "are unsure about an answer, truthfully say \"I don't know\" <>  Explain artificial intelligence in a\n",
            "few lines [/INST]  Sure! Here's my explanation of artificial intelligence:  Artificial intelligence\n",
            "(AI) refers to the development of computer systems that can perform tasks that typically require\n",
            "human intelligence, such as learning, problem-solving, and decision-making. These systems use\n",
            "algorithms and machine learning techniques to analyze data and make predictions or take actions\n",
            "based on that data. Some examples of AI include natural language processing, image recognition, and\n",
            "autonomous vehicles. Overall, AI is a rapidly advancing field that has the potential to transform\n",
            "many industries and aspects of our lives.\n"
          ]
        }
      ],
      "source": [
        "from textwrap import fill\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "[INST] <>\n",
        "You are an expert in Mars survival, drawing from accurate, comprehensive information. Use the following pieces of context to answer the question at the end, and create practical scenarios when relevant. If you don't know the answer, say \"I don't know,\" and avoid speculation. Keep the answers informative but concise, and include key survival tips when applicable. Always end with \"Thanks for asking!\".\n",
        "<>\n",
        "\n",
        "{text} [/INST]\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "text = \"Explain artificial intelligence in a few lines\"\n",
        "result = llm.invoke(prompt.format(text=text))\n",
        "print(fill(result.strip(), width=100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5einWLF-oFXA"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PX9ybmOGoTvK"
      },
      "source": [
        "# **RAG from PDF Files**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvpSN5ILyjCp"
      },
      "source": [
        "*A. Create a vectore store for the context/external data.*\n",
        "*Here, we'll create embedding vectores of the unstructured data loaded from the the source and store them in a vectore store.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPGA-bfKqU8C"
      },
      "source": [
        "**Load PDF Files**\n",
        "\n",
        "*Depending on the type of the source data, we can use the appropriate data loader from LangChain to load the data.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKSoiF59obOc"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import UnstructuredPDFLoader\n",
        "from langchain.vectorstores.utils import filter_complex_metadata # 'filter_complex_metadata' removes complex metadata that are not in str, int, float or bool format\n",
        "\n",
        "pdf_loader = UnstructuredPDFLoader(\"\")\n",
        "pdf_doc = pdf_loader.load()\n",
        "updated_pdf_doc = filter_complex_metadata(pdf_doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oB7tJYRqtUN"
      },
      "source": [
        "**Spit the document into chunks**\n",
        "\n",
        "*Due to the limited size of the context window of an LLM, the data need to be divided into smaller chunks with a text splitter like CharacterTextSplitter or RecursiveCharacterTextSplitter. In this way, the smaller chunks can be fed into the LLM.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beX43Y5gq5NT",
        "outputId": "c3e7f83b-1688-4c78-b689-ff2469a137c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "311"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=128)\n",
        "chunked_pdf_doc = text_splitter.split_documents(updated_pdf_doc)\n",
        "len(chunked_pdf_doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iB5TA12MrQxg"
      },
      "source": [
        "**Create a vector database of the chunked documents with HuggingFace embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425,
          "referenced_widgets": [
            "a1eccc7d2b8b425283cb94a8683344ba",
            "b1d9870c5dc94a4185f5330fa47cf76d",
            "4d8731f6cbf345199759e3b7b663e2b3",
            "41aa7a092c7b4fca96c3dcaa02f7be82",
            "0c005e9afcf64522ae3b3d232cadb033",
            "f0de8def13e24d2fae1b130c4fc3388d",
            "a27d65ae4e8a4d9b942fc0cd755433ad",
            "ac1169b691034922b27d9352c40313ea",
            "fd6fc4eaa7354128a962786cf740871d",
            "c4acb7a8f58a4f9aa8ca48863ac00269",
            "445fef6dbc4f448fbefe048aa2accb56",
            "2b8142ead65043fd9ba3f8d31a86397c",
            "39e5f1e888b24aecad546bccd14800ff",
            "c0c1fff3ea824f04846120170c568717",
            "aeec06027db74fcbb1a7a33014dbda21",
            "e5e62555d8844d4a8085cc849b1afb0c",
            "54734d6dccbf4a54920e614fbc1a229e",
            "bf56ebf61c5e44dbaf3f73c4769a60f6",
            "f3b7110d74c447ca9a15fcbc1885017b",
            "fbe5951f79f44a9db4c94e46560d5532",
            "9955eac69d394ef9916697f23dc3f2dd",
            "2629ad38c784446c8895a3fe0cfaee75",
            "303b7cdec792472eaf07a5557dd0ae76",
            "dd4eefc2b73d4cddbed983e6ae699f72",
            "21a50e4f97c844a8b855c4f5789055a2",
            "1d8cdc95fdce441aa6f436d070cd14c8",
            "a7c9503ba350446aa822e9448e17b94c",
            "afc152643cc24aebbea58b8c4448952f",
            "6a83d61a657d491f99abbdc71fa16d96",
            "b21fe44078b64db980d5ca14be81dfca",
            "e8e9c2a339244e1d9f35716224f8625d",
            "eeccf5709c244e1790b89bbff5a880d6",
            "e055dde9bfbd4592b12bfbb94e9102e9",
            "4f04aae222484ff68a612e16d06e219e",
            "525cd06597894b9d85adc9f7e37510df",
            "a8543bf7522a47abbf4c1c15ed8fab07",
            "4087af8b56544382a4b7a11fdc4c0304",
            "1eb8b3091b18421eaaa63c72d3392d84",
            "70ef74d69cfc4797a9d9f65e0fd6a362",
            "6edc56f6fb1c49949c8cb7b97b918565",
            "ea9533d47e504078b6485bbe3bae03bf",
            "0a0baca6213d402faf22db6b883178dc",
            "ebc182bcf717490a8e5f57e0739dfeb0",
            "e9f7086444914ae9bc79f6c9631ea11a",
            "170ebfe3f3094b0eb45ce6278a76cc94",
            "a922691caa96450db8fdbd523a53c032",
            "0651bb83f18944a2886e4649ab6d73e9",
            "f939b84275ea47499d7b5cf4cd278cd9",
            "a0475c3a61eb44c8b4421f88d9fe2821",
            "e4a42e186bcd42d4b784fafcce8cb761",
            "ab47a7615fee4df780e07e30dde18501",
            "bf013a63b9d64920a873aa418e0e8ff0",
            "9acbcb21cba447a9ae90908e6b099b93",
            "f98cca08483d475489dd09417f9512f7",
            "b4193422b3574ab4a387641f939c881a",
            "88705b62cae2413d82461ffb894d3572",
            "836a3df9e2624a5e8ec0fab9493b3c33",
            "f0dab89206d7449caac39977e554ca91",
            "0caddbc138984123a21b4fd0cc0b042c",
            "063d27cfe6cf4c87badbe4885db1bd5b",
            "5385140f06f147d49bee36175dd58bac",
            "0e5517f5630d40d2afe0aa23b4e18a92",
            "a28f93cfbc574bcf8c065098c15fcb4f",
            "12f794d798e74fa0892d6216efebe83e",
            "b707de69afbe494cb5c4e1277e99c069",
            "5795a4a90a804334af3a8688205a0d86",
            "055cfa922a8e4f509cc85faafbf20c47",
            "a00e8df903a84746b7056740d0db4618",
            "32a14c9eb925461aa1c5f3a37e8f4835",
            "b2fa5838ef944ca19621480dd1aa44f3",
            "719270f90c72472a968ee77ddcf0c596",
            "9486f31be66f46c996cf8d80bc2f2410",
            "1bfcedba264b4167a651936d0160ba6c",
            "6af76c514e53467baf5d52cdd4be00d6",
            "f579880f434a4be5ac5117abbd970950",
            "b9372a46fd2f41fda70ed89524b9fcf7",
            "d54f392c7bf54b408c0d1bf30e019afa",
            "0fb2acca826247489eddfe950835cb4f",
            "89a12709d0ba4b5dae12b2f4848bc01d",
            "d59a2ab6791e4ec3859e84ca8aaf1e85",
            "e667e5c592b8404091e7c413bef96540",
            "2a4b57cce3744e2296d4fce9ff823b31",
            "58cc59387e59410f8e1125657560dc23",
            "707d39a7daa24d469e14bb507201f00f",
            "31e2be49265a4b4d918df890b216ad49",
            "474296c8c73741a1962df9ab317e5968",
            "ae3318fa3d3641e18b741ce95fab2b3b",
            "10b194edec5140baad3c490d12b6b2ad",
            "2da3b90a846f44d19d533e495c90861e",
            "025dbc7047cd4c788be5d25312427d73",
            "7d5fe1a3868147b1a0b936342c51fde6",
            "d565be9ceb6d4e808a36d24a3fc01aa8",
            "3e7ff147d30a44409a6dc87e84f66a5c",
            "2856131adade4194a2d6764c85319705",
            "8266c93ffdf847e09d98317fee365b87",
            "02c960301b2a4c5dbf7276839ad826d8",
            "1e1c867b1c6e40b584537e8df5ad70b0",
            "7161c379ec77440b87d9c44f96c6fcbd",
            "5cb9cf8aa8ba40b899b2706eab83120e",
            "7b8df8c870b740519eec788af0439879",
            "0ae31c5d827b4b55aeab09804638ee17",
            "18c41548f8e44cf397c97a4190830368",
            "069d89f3dbcc480b9b85a0e493dd690f",
            "7c48e8f0467a454dba3ba25e0d8d41ac",
            "ac2f03e77dde42bfbcdf53056ccd7bea",
            "bdf0d5db3379400dab8bc5787f8802ed",
            "9d2f12cc53b044bfa0aa3f4f06bb2783",
            "7b0aa6b703344fc48664dc371e6bf839",
            "4c835faf0b61461caec1314069dfd2f2",
            "db604fcba5814f6294da32ee4b158ba4",
            "188125bb10d64d21a4330550a6a8bc74",
            "e3de206bcd93405594bc5807ae139b7d",
            "cb8d33cadc264e1e9411b2d3c5ac7649",
            "af09717f1c5846d0972efc7faeeeb9e4",
            "917d0b0d95684914847aa3fbead8daff",
            "a3e8f66f092a47d48051dcc0c0d80b5a",
            "19a340a49d0d47ddb0df0eac6b44577e",
            "3ca1999ceb1b471b9de2ac02aebfd792",
            "757149db0c2a4b128b5c0e3ae627aabb",
            "592a4de254a046b18a319117e1b48f9d",
            "1b68e3b43cd14233af50bb3f4523d265"
          ]
        },
        "id": "vqEdC08VrQBA",
        "outputId": "e160cf99-d072-4510-9803-d3cadb1e948d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a1eccc7d2b8b425283cb94a8683344ba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b8142ead65043fd9ba3f8d31a86397c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "303b7cdec792472eaf07a5557dd0ae76",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4f04aae222484ff68a612e16d06e219e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "170ebfe3f3094b0eb45ce6278a76cc94",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "88705b62cae2413d82461ffb894d3572",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "055cfa922a8e4f509cc85faafbf20c47",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0fb2acca826247489eddfe950835cb4f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2da3b90a846f44d19d533e495c90861e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b8df8c870b740519eec788af0439879",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "188125bb10d64d21a4330550a6a8bc74",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4O2SUJ9GrdbX"
      },
      "source": [
        "*We can either use FAISS or Chroma to create the Vector Store.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2BGTg6MrVjS",
        "outputId": "a1eb72c5-d6c0-4183-b214-996db43f5bd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 8.45 s, sys: 242 ms, total: 8.7 s\n",
            "Wall time: 10.2 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Create the vectorized db with FAISS\n",
        "from langchain.vectorstores import FAISS\n",
        "db_pdf = FAISS.from_documents(chunked_pdf_doc, embeddings)\n",
        "\n",
        "# Create the vectorized db with Chroma\n",
        "# from langchain.vectorstores import Chroma\n",
        "# db_pdf = Chroma.from_documents(chunked_pdf_doc, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OSsQd-5rpZl"
      },
      "source": [
        "*B. Use RetrievalQA chain*\n",
        "\n",
        "*We instantiate a RetrievalQA chain from LangChain which takes in a retriever, LLM and a chain_type as the input arguments. When the QA chain receives a query, the retriever retrieves information relevent to the query from the vectore store. The chain type = \"stuff\" method stuffs all the retrieved information into context and makes a call to the language model. The LLM then generates the text/response from the retrieved documents. See information on Langchain Retriver.*\n",
        "\n",
        "**LLM prompt structure**\n",
        "\n",
        "*We can also pass in the recommended prompt structue for Llama 2 for the QA. In this way, we'd be able to advise our LLM to only use the available context to answer our question. If it cannot find information relevant to our query in the context, it'll NOT make up an answer, rather, it would advise that it's unable to find relevant information in the context.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSmUWBWnra0W",
        "outputId": "e613d4e7-3f0c-48e7-f57d-e22068887246"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INST] <> Use the following context to Answer the question at the end. Do not use any other\n",
            "information. If you can't find the relevant information in the context, just say you don't have\n",
            "enough information to answer the question. Don't try to make up an answer.  <>  importants,\n",
            "dateReceptionOffres:  2024-09-12T10:  00:  00,  nbCandidatReduit:  non:,\n",
            "attributionSansNegociation: oui:, procedureAdapteeO:, capaciteEcoFin:- Déclaration appropriée de\n",
            "banques ou preuve d'une assurance pour les risques professionnels.- Déclaration concernant le\n",
            "chiffre d'affaires global et le chiffre d'affaires concernant les fournitures, services ou travaux\n",
            "objet du  marché, réalisés au cours des  trois derniers exercices disponibles,  techAchat: autres:,\n",
            "CSV to PDF  natureMarche: tranches: non:, reservationMarche: non:, lotsMarche: oui:, codeCPV:\n",
            "objetPrincipal:  classPrincipale: 45262700, dureeMois: 10, description: lt;br/gt;Chaque lot fera\n",
            "l'objet d'une  attribution séparée.lt;br/gt;Les prestations sont réglées par des prix\n",
            "forfaitaires.lt;br/gt;lt;br/gt;,  lieuExecution:  Lieu-dit  42120- NOTRE-DAME-DE-BOISSET,  intitule:\n",
            "TRAVAUX DE  REHABILITATION D'UNE MAISON EN PIERRE POUR LA CREATION DE BUREAUX SUR LA\n",
            "identifiantInterne: 24S0060, procedure: capaciteExercice:- Les pièces prévues aux articles D.\n",
            "8222-5 ou D. 8222-7 et D. 8222-8 du code du travail- Déclaration sur l'honneur du candidat attestant\n",
            "qu'il est en règle, au cours de l'année précédant celle au cours de laquelle a lieu le lancement de\n",
            "la  consultation, au regard des articles L. 5212-1, L. 5212-2, L. 5212 5 et L. 5212-9 du code du\n",
            "travail,  concernant l'emploi des travailleurs handicapés- Si le candidat est établi en France, une\n",
            "déclaration  sur l'honneur du candidat justifiant que le travail est effectué par des salariés\n",
            "employés  régulièrement au regard des articles L. 1221-10, L. 3243-2 et R. 3243-1 du code du\n",
            "travail(dans le  CSV to PDF  cas où le candidat emploie des salariés, conformément à l'article D.\n",
            "8222-5-3 du code du travail) 1/  situation  juridique-  références  requises: Déclaration sur\n",
            "l'honneur pour  justifier que  soumissionnaire ne relève pas d'un motif d'exclusion de la procédure\n",
            "de passation mentionné aux  concernant le chiffre d'affaires global et le chiffre d'affaires\n",
            "concernant les fournitures, services ou  travaux objet du marché,  réalisés au cours des  trois\n",
            "derniers exercices disponibles,  nbCandidatReduit: non:, presOffres: interdite:,\n",
            "attributionSansNegociation: oui:, variantes: non:,  categorieAcheteur: Toute personne désignée à cet\n",
            "effet par l'acheteur, procedureAdapteeO:,  capaciteTech:- Présentation d'une liste des principales\n",
            "fournitures ou des principaux services  CSV to PDF  effectués au cours des trois dernières années,\n",
            "indiquant le montant, la date et le destinataire public  ou privé.- Déclaration indiquant les\n",
            "effectifs moyens annuels du candidat et l'importance du  personnel d'encadrement pour chacune des\n",
            "trois dernières années, techAchat: accordCadre:,  dateReceptionOffres: 2024-08-02T16: 00: 00,\n",
            "criteresAttrib: Valeur technique: 60 Prix: 40,  natureMarche: intitule: Acquisition et mise en\n",
            "oeuvre d'une solution de coffre-fort numérique pour  selection-criterion, text: ef-stand, cbc:\n",
            "Description: text:- Déclaration concernant le chiffre d'affaires  global et le chiffre d'affaires\n",
            "concernant les fournitures, services ou travaux objet du marché,  réalisés au cours des  trois\n",
            "derniers exercices disponibles,  languageID: FRA, cbc:  CalculationExpressionCode:  listName:\n",
            "usage,  text:  used,,  cac: AppealTerms:  cac:  AppealReceiverParty: cac: PartyIdentification: cbc:\n",
            "ID: text: ORG-0003, schemeName: organization,  cac: AppealInformationParty: cac:\n",
            "PartyIdentification: cbc: ID: schemeName: organization, text:  ORG-0004, cac: Language: cbc: ID:\n",
            "FRA, cbc: FundingProgramCode: listName: eu-funded, text:  no-eu-funds, cac:\n",
            "CallForTendersDocumentReference: cbc: ID: 1496889, cbc: DocumentType:  non-restricted-document, cbc:\n",
            "LanguageID: FRA, cbc: DocumentStatusCode:  listName:  linguistic-status,  text: official,  cac:\n",
            "Attachment:  cac: ExternalReference:  cbc: URI:  https://www.marches-\n",
            "publics.info/mpiaws/index.cfmfuseactiondematEnt.loginamp;typeDCEamp;IDM  Question: What is the\n",
            "context about? [/INST]  The context is about a public tender for the acquisition and implementation\n",
            "of a digital safe solution. The tender is open to all natural persons designated by the buyer, and\n",
            "the contract will be awarded without negotiation. The tenderer must provide a declaration on honor\n",
            "attesting that they are in good standing and have not been excluded from the procurement procedure.\n",
            "The tenderer must also provide information on their financial capacity, technical capabilities, and\n",
            "experience in providing similar solutions. The tenderer must declare that they have not been\n",
            "penalized for professional misconduct and that they have not been the subject of any legal\n",
            "proceedings. The context also includes information about the tender's language, currency, and\n",
            "location.\n",
            "CPU times: user 2min 18s, sys: 1min 32s, total: 3min 50s\n",
            "Wall time: 3min 59s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# use the recommended propt style for the LLAMA 2 LLM\n",
        "prompt_template = \"\"\"\n",
        "[INST] <>\n",
        "You are an expert in Mars survival, drawing from accurate, comprehensive information. Use the following pieces of context to answer the question at the end, and create practical scenarios when relevant. If you don't know the answer, say \"I don't know,\" and avoid speculation. Keep the answers informative but concise, and include key survival tips when applicable. Always end with \"Thanks for asking!\".\n",
        "<>\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question} [/INST]\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "Chain_pdf = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    # retriever=db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={'k': 5, 'score_threshold': 0.8})\n",
        "    # Similarity Search is the default way to retrieve documents relevant to a query, but we can use MMR by setting search_type = \"mmr\"\n",
        "    # k defines how many documents are returned; defaults to 4.\n",
        "    # score_threshold allows to set a minimum relevance for documents returned by the retriever, if we are using the \"similarity_score_threshold\" search type.\n",
        "    # return_source_documents=True, # Optional parameter, returns the source documents used to answer the question\n",
        "    retriever=db_pdf.as_retriever(), # (search_kwargs={'k': 5, 'score_threshold': 0.8}),\n",
        "    chain_type_kwargs={\"prompt\": prompt},\n",
        ")\n",
        "query = \"What is the context about?\"\n",
        "result = Chain_pdf.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dbd0jW74tfl6"
      },
      "source": [
        "*let's try with another document.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8rN9qAjsLAM"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import UnstructuredPDFLoader\n",
        "from langchain.vectorstores.utils import filter_complex_metadata # 'filter_complex_metadata' removes complex metadata that are not in str, int, float or bool format\n",
        "\n",
        "pdf_loader = UnstructuredPDFLoader(\"/content/MarsReview.pdf\")\n",
        "pdf_doc = pdf_loader.load()\n",
        "updated_pdf_doc = filter_complex_metadata(pdf_doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5oubpqetkBv",
        "outputId": "ef712d34-f5ba-4ccc-ecbb-65379dd125cc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1447"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=128)\n",
        "chunked_pdf_doc = text_splitter.split_documents(updated_pdf_doc)\n",
        "len(chunked_pdf_doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKwYatMgttOL",
        "outputId": "15ea29ed-fa99-48b5-d605-ac21c501883b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 31.6 s, sys: 467 µs, total: 31.6 s\n",
            "Wall time: 34.6 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Create the vectorized db with FAISS\n",
        "from langchain.vectorstores import FAISS\n",
        "db_pdf = FAISS.from_documents(chunked_pdf_doc, embeddings)\n",
        "\n",
        "# Create the vectorized db with Chroma\n",
        "# from langchain.vectorstores import Chroma\n",
        "# db_pdf = Chroma.from_documents(chunked_pdf_doc, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6uX33Xytw7c",
        "outputId": "be5b4b23-4e18-4212-e8e2-747d2a5fe3b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INST] <> Use the following context to Answer the question at the end. Do not use any other\n",
            "information. If you can't find the relevant information in the context, just say you don't have\n",
            "enough information to answer the question. Don't try to make up an answer.  <>  des marchés du\n",
            "CHTMMB de Mbour au plus tard le Mardi 14 Mai 2024 à 10heures 00mn. Les offres  soumises après la\n",
            "date et heure limites de dépôt des offres, ne seront pas acceptées. Les offres  seront ouvertes en\n",
            "présence des représentants des candidats présents à l'adresse ci-après au  bureau du chef du service\n",
            "administratif et Financier le Mardi 14 Mai 2024 à. 10heures 00mn. Les  offres doivent comprendre une\n",
            "garantie de soumission, d'un montant de: Lot1: Fourniture de fils de  suture pour chirurgie\n",
            "classique: 330 000 FCFA; Lot 2: Fourniture de fils de suture spéciaux et  ligatures: 45 000 FCFA;\n",
            "Lot 3: Fourniture de dispositifs médicaux pour l'anesthésie-réanimation, les  interventions\n",
            "d'urgence: 180 000 FCFA; Lot 4: Produits et consommables pourl'odontologie: 180  o00 FCFA. Les\n",
            "offres demeureront valides pendant une durée de quatre-vingt-dix(90) jours à  CSV to PDF  compter de\n",
            "la date limite de soumission. Description de la société LA SANTE, UN DROIT. LE  suture pour\n",
            "chirurgie classique: 3 666 660 FCFA; Lot 2: Fourniture de fils de suture spéciaux et  ligatures: 500\n",
            "000 FCFA; Lot 3: Fourniture de dispositifs médicaux pour l'anesthésie-réanimation,  CSV to PDF  les\n",
            "interventions d'urgence: 2 000 000 FCFA; Lot 4:'Produits et consommables pour l'odontologie: 2  000\n",
            "000 FCFA Expérience et Capacité technique Un(01) marché similaire, pour le lot à  soumissionner,\n",
            "exécuté durant les deux(02) dernières années(2023-2024) avec une attestation de  service fait\n",
            "délivrée par le service bénéficiaire. Les attestations à cet effet doivent préciser l'autorité\n",
            "bénéficiaire, l'ob-jet, le montant et l'année du marché. Les pièces administratives: Lettre de\n",
            "soumission, Charte de transparence et d'éthique. en matière de marchés publics, Attestation de\n",
            "l'IPRES, Quitus fiscal, Attestation de la Caisse de Sécurité Sociale, Attestation de l'Inspection du\n",
            "Travail, attestation de redevance a l'ARMP, Registre de commerce, NINEA, déclaration sur  Mansour\n",
            "Barro de Mbour dispose de crédits dans le cadre du budget 2024 afin de financer la  fourniture de\n",
            "médicaments et produits médico-chirurgicaux et a l'intention d'utiliser une partie de ces\n",
            "ressources pour effectuer des paiements au titre du marché de F-CHTMMBM N007/2024. 3. Le  Centre\n",
            "Hospitalier Thierno Mouhamadoul Mansour Barro de Mbour sollicite des offres sous pli fermé  de la\n",
            "part de candidats éligibles et répondant aux qualifications requises pour la fourniture de\n",
            "médicaments et produits médico-chirurgicaux L'Appel d'Offres porte sur quatre(04) lots: Lot 1:\n",
            "Fourniture de fils de suture pour chirurgie classique; Lot 2: Fourniture de fils de suture spéciaux\n",
            "et  ligatures; Lot 3: Fourniture de dispositifs médicaux pour l'anesthésie-réanimation, les\n",
            "interventions  d'urgence; Lot 4: Produits et consommables pour l'odontologie: 2 000 000 FCFA. Ces\n",
            "produits sont  celles définies dans les lots ci-dessus et servent à approvisionner la pharmacie du\n",
            "CHTMMB de  Date: 29 mai 2024 à 20h50  Category: Médecine-Santé  Location: Saint-Louis  Post Type:\n",
            "Appel d'offre  Company: CENTRE HOSPITALIER RÉGIONAL LIEUTENANT-COLONEL MAMADOU DIOUF DE  SAINT-LOUIS\n",
            "Description: Détails de l'annonce RÉPUBLIQUE DU SÉNÉGALMINISTÈRE DE LA SANTÉ, ET DE  L'ACTION\n",
            "SOCIALEDIRECTION DES ÉTABLISSEMENTS DE SANTÉCENTRE HOSPITALIER  RÉGIONAL LIEUTENANT-COLONEL MAMADOU\n",
            "DIOUF DE SAINT-LOUISAVIS DE DEMANDE  DE RENSEIGNEMENTS ET DE PRIX OUVERTE AOO\n",
            "06/2024_F_CHRSLacquisition  d'équipements biomédicaux et matériels médicaux 1. Cet Avis d'appel\n",
            "d'offres ouvert falt suite à  CSV to PDF  l'Avis Général de Passation des Marchés paru dans le\n",
            "quotidien le Soleil Edition n'16056 du  vendredi o8 décembre 2023. 2. Le Centre hospitaller régional\n",
            "Lieutenant-colonel Mamadou DIOUF  de Saint-Louis(CHRLCMD/SL) a obtenu, dans le cadre de son budget\n",
            "d'investissement, des fonds  afin de financer les fournitures diverses et a l'intention d'utiliser\n",
            "une partie de ces fonds pour  Question: What are the best tenders? [/INST]  Based on the information\n",
            "provided, there are four lots available for tender:  1. Lot 1: Fourniture de fils de suture pour\n",
            "chirurgie classique (330,000 FCFA) 2. Lot 2: Fourniture de fils de suture spéciaux et ligatures\n",
            "(45,000 FCFA) 3. Lot 3: Fourniture de dispositifs médicaux pour l'anesthésie-réanimation et les\n",
            "interventions d'urgence (180,000 FCFA) 4. Lot 4: Produits et consommables pour l'odontologie\n",
            "(180,000 FCFA)  The tender is open to eligible candidates who have experience in providing similar\n",
            "products and services within the last two years (2023-2024). The tender documents include a letter\n",
            "of submission, charter of transparency and ethics, and attestations from the IPRES, Quitus fiscal,\n",
            "CASS, INSEE, and ARMP. The deadline for submission is Tuesday, May 14, 2024, at 10:00 am.\n",
            "CPU times: user 4min 11s, sys: 2min 50s, total: 7min 1s\n",
            "Wall time: 7min 8s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# use the recommended propt style for the LLAMA 2 LLM\n",
        "prompt_template = \"\"\"\n",
        "[INST] <>\n",
        "You are an expert in Mars survival, drawing from accurate, comprehensive information. Use the following pieces of context to answer the question at the end, and create practical scenarios when relevant. If you don't know the answer, say \"I don't know,\" and avoid speculation. Keep the answers informative but concise, and include key survival tips when applicable. Always end with \"Thanks for asking!\".\n",
        "<>\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question} [/INST]\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "Chain_pdf = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    # retriever=db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={'k': 5, 'score_threshold': 0.8})\n",
        "    # Similarity Search is the default way to retrieve documents relevant to a query, but we can use MMR by setting search_type = \"mmr\"\n",
        "    # k defines how many documents are returned; defaults to 4.\n",
        "    # score_threshold allows to set a minimum relevance for documents returned by the retriever, if we are using the \"similarity_score_threshold\" search type.\n",
        "    # return_source_documents=True, # Optional parameter, returns the source documents used to answer the question\n",
        "    retriever=db_pdf.as_retriever(), # (search_kwargs={'k': 5, 'score_threshold': 0.8}),\n",
        "    chain_type_kwargs={\"prompt\": prompt},\n",
        ")\n",
        "query = \"What are the best tenders?\"\n",
        "result = Chain_pdf.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_--7AtZwEbw"
      },
      "source": [
        "***As we can see, we clearly have a problem in terms of running time.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XApnHTw6wV-r"
      },
      "source": [
        "*C. Hallucination Check*\n",
        "\n",
        "*Hallucination in RAG refers to the generation of content by an LLM that is not based onn the retrieved knowledge.*\n",
        "\n",
        "*Let's test our LLM with a query that is not relevant to the context. The model should respond that it does not have enough information to respond to this query.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwPRHgDKuFh8",
        "outputId": "3944115a-b846-44fa-d47c-0567fccb46ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INST] <> Use the following context to Answer the question at the end. Do not use any other\n",
            "information. If you can't find the relevant information in the context, just say you don't have\n",
            "enough information to answer the question. Don't try to make up an answer.  <>  conduite par Appel\n",
            "d'Offre Ouvert, telle que définie dans le code des Marchés Publics et ouverte à  tous les candidats\n",
            "éligibles. 5. Les candidats intéressés peuvent obtenir des informations auprès du  secrétariat de la\n",
            "Direction du Centre Hospitalier National Cheikh Ahmadoul Khadim(Touba Route de  Darou Mousty, BP\n",
            "2,téléphone 33 885 11 38), et prendre connaissance des documents de I'AOO. 6.  Les exigences en\n",
            "matière de qualification sont: Capacité financière:- Avoir un chiffre d'affaires  annuel moyen égal\n",
            "à cent vingt millions(157 500 000) f CFA. A cet effet, il convient de Fournir les  états financiers\n",
            "des trois derniers exercices(2020,2021 et 2022) certifiés par un expert-comptable  agréé par\n",
            "l'ONECCA justifiant une assise financière solide du candidat. Capacité technique et\n",
            "expérience:-Avoir réalisé pour chaque lot au moins un marché de natures similaire durant les trois\n",
            "dernières années(2021,2022 et 2023).Joindre l'attestation de service fait signée par l'autorité\n",
            "Amadou Mahtar MBOW; roe 21x20, 2ème arrondissement, pôle urbain de Diamniadio en face\n",
            "stadeAbdoulaye WADE, le 13 juin 2024 à 10h35. Les offres doivent comprendre une garantie de\n",
            "soumission d'un montant d'un million(1.000 000) F CFA pour le lot1 uniquement, il n'est pas exigé\n",
            "une garantie de soumission pour le lot 2Les offres demeureront valides pendant une durée de  quatre-\n",
            "vingt-dix-neuf(99) Jours à compter de date de soumission LE RECTEUR Description de la  société\n",
            "Université Amadou Mahtar Mbow(UAM) Université d'innovation au service du  développement. pôle urbain\n",
            "de Diamniadio en face du stade Abdoulaye WADE www.uam.sn  Contact Information: REPUBLIQUE DU\n",
            "SENEGALUn Peuple- Un But Une Foi Ministère de  lEnseignement Supérieur, de la Recherche et de\n",
            "linnovation Université Amadou Mahtar MBOW Avis  dAppel dOffres National(AON) AO F_UAMREC_008 1. Cet\n",
            "Avis d'appel d'offres fait suite à l'Avis  Général de Passation des Marchés paru dans le quotidien\n",
            "Le Soleil n'16080 du 08 Janvier 2024 et  à 13h47 3 juin 2024 à 16h32 3 juin 2024 à 15h22 3 juin 2024\n",
            "à 12h52 3 juin 2024 à 11h35 3 juin  2024 à 10h48 31 mai 2024 à 12h11 31 mai 2024 à 11h38  Category:\n",
            "Fournitures courantes Fournitures courantes Prestations intellectuelles Fournitures  courantes\n",
            "Prestations intellectuelles Prestations intellectuelles Fournitures courantes Fournitures  courantes\n",
            "Prestations intellectuelles Fournitures courantes Fournitures courantes Fournitures  courantes\n",
            "Location: Dakar Dakar Dakar Dakar Dakar Dakar Dakar Dakar Tambacounda Dakar Thiès Dakar  Post Type:\n",
            "Appel d'offre Appel d'offre Appel d'offre Appel d'offre Appel d'offre Appel d'offre Appel  d'offre\n",
            "Appel d'offre Appel d'offre Appel d'offre Appel d'offre Appel d'offre  Company: HOPTAL PRINCIPAL DE\n",
            "DAKAR(HPD) AGENCE SENEGALAISE D'ETUDE  SPATIALES(ASES) Direction Des Systèmes D'Information\n",
            "Ministère De L'Urbanisme, Des  Collectivités Territoriales Et De L'Aménagement Du Territoire\n",
            "MINISTÈRE DE LA  COMMUNICATION, DES TÉLÉCOMMUNICATIONS ET DU NUMÉRIQUE SENELEC Ecole  passation des\n",
            "marchés de Université Amadou Makhtar MBOW; sis au 1er étage duRectorat de  l'Université Amadou\n",
            "Mahtar MBOW sis rue 21x20, 2ème arrondissement, pôle urbain de Diamniadio  en face du stade\n",
            "Abdoulaye WADE, tel: 76 623 73 33, email: cpm.uamuam.edu.sn et prendre  connaissance des documents\n",
            "d'Appel d'offres à l'adresse mentionnée ci-dessus du lundi au  vendredi de o8 h à 16h. 6. Les\n",
            "exigences en matière de qualification sont: Le candidat doit: a. Avoir  une capacité\n",
            "financière,-disposer d'une ligne de crédit d'un montant égal à la moitié de son offre  délivrée par\n",
            "un établissement ou organisme financier agrée par le Ministère de l'Économie,des  Finances et du\n",
            "Plan.-fournir les états financiers certifiés des années 2020, 2021 et 2022.b. Avoir  une capacité\n",
            "technique et expérience,-apporter la preuve d'avoir réalisé avec satisfaction des  acheteurs, au\n",
            "cours des années 2020, 2021 et 2022 au moins un marché de même nature et/ou de  Question: who is\n",
            "Cristiano Ronaldo? [/INST]  I cannot answer this question as there is no information provided about\n",
            "Cristiano Ronaldo in the given text. The text only discusses an appeal for tenders and the\n",
            "requirements for candidates interested in bidding. Therefore, I do not have enough information to\n",
            "answer the question about Cristiano Ronaldo.\n",
            "CPU times: user 1min, sys: 40.9 s, total: 1min 41s\n",
            "Wall time: 1min 43s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "query = \"who is Cristiano Ronaldo?\"\n",
        "result = Chain_pdf.invoke(query)\n",
        "print(fill(result['result'].strip(), width=100))"
      ]
    }
  ]
}